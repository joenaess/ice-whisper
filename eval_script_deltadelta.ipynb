{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ad8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, DatasetDict, Dataset, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainingArguments, # Used indirectly for device placement helper\n",
    "    Seq2SeqTrainer # Used indirectly for device placement helper\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm # Use standard tqdm if not in notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afbe667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using model precision: torch.bfloat16\n",
      "Evaluating on: language-and-voice-lab/althingi_asr (Split: validation, Subset Size: 1000)\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "MODEL_ID = \"language-and-voice-lab/whisper-large-icelandic-30k-steps-1000h\"\n",
    "# !! MODIFY THIS PATH !!\n",
    "ADAPTER_CHECKPOINT_PATH = \"./whisper-lvl-base-raddromur-lora/checkpoint-216\" # Example path, use your checkpoint\n",
    "# !! ------------- !!\n",
    "\n",
    "# Evaluation Dataset Configuration (Althingi ASR - Validation Split)\n",
    "EVAL_DATASET_ID = \"language-and-voice-lab/althingi_asr\"\n",
    "EVAL_SPLIT = \"validation\" # As requested\n",
    "EVAL_SUBSET_SIZE = 1000 # Number of samples to randomly select for evaluation (adjust as needed for Althingi)\n",
    "\n",
    "TARGET_LANGUAGE = \"is\"\n",
    "TASK = \"transcribe\"\n",
    "MODEL_PRECISION = torch.bfloat16 # Use the same precision as training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EVAL_BATCH_SIZE = 1 # Adjust based on GPU memory during evaluation\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Using model precision: {MODEL_PRECISION}\")\n",
    "print(f\"Evaluating on: {EVAL_DATASET_ID} (Split: {EVAL_SPLIT}, Subset Size: {EVAL_SUBSET_SIZE})\")\n",
    "\n",
    "# --- Load Metrics ---\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64e31e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading processor for language-and-voice-lab/whisper-large-icelandic-30k-steps-1000h...\n",
      "Processor loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Text Normalization Function ---\n",
    "# Samromur 'normalized_text' is already lowercase, no punctuation/digits.\n",
    "# This function primarily handles potential extra whitespace.\n",
    "chars_to_remove_regex = r'[<\\[\\].,?!\\-:;\"]' # Keep for Whisper output normalization\n",
    "\n",
    "def normalize_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(chars_to_remove_regex, '', text).strip()\n",
    "    return text\n",
    "\n",
    "# --- Load Processor ---\n",
    "print(f\"\\nLoading processor for {MODEL_ID}...\")\n",
    "try:\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_ID, language=TARGET_LANGUAGE, task=TASK)\n",
    "    print(\"Processor loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processor: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b0e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading evaluation dataset language-and-voice-lab/althingi_asr split 'validation'...\n",
      "Full split 'validation' loaded with 5504 samples.\n",
      "Selecting random subset of 1000 samples...\n",
      "Subset selected.\n",
      "Preprocessing evaluation subset...\n",
      "Dataset subset preprocessed.\n",
      "Example reference: við teljum í vestnorræna ráðinu að þessi atriði sem talin eru upp\n"
     ]
    }
   ],
   "source": [
    "# --- Load Evaluation Dataset ---\n",
    "print(f\"\\nLoading evaluation dataset {EVAL_DATASET_ID} split '{EVAL_SPLIT}'...\")\n",
    "try:\n",
    "    # Load the specific demographic split\n",
    "    eval_dataset_full_split = load_dataset(EVAL_DATASET_ID, split=EVAL_SPLIT)\n",
    "    print(f\"Full split '{EVAL_SPLIT}' loaded with {len(eval_dataset_full_split)} samples.\")\n",
    "\n",
    "    # --- Select a Random Subset ---\n",
    "    if EVAL_SUBSET_SIZE > len(eval_dataset_full_split):\n",
    "        print(f\"Warning: Requested subset size {EVAL_SUBSET_SIZE} is larger than the split size {len(eval_dataset_full_split)}. Using full split.\")\n",
    "        eval_dataset = eval_dataset_full_split\n",
    "    else:\n",
    "        print(f\"Selecting random subset of {EVAL_SUBSET_SIZE} samples...\")\n",
    "        eval_dataset = eval_dataset_full_split.shuffle(seed=42).select(range(EVAL_SUBSET_SIZE))\n",
    "        print(f\"Subset selected.\")\n",
    "\n",
    "    # --- Preprocess Evaluation Dataset ---\n",
    "    print(\"Preprocessing evaluation subset...\")\n",
    "    # Keep relevant columns: 'audio', 'normalized_text'\n",
    "    reference_col = \"normalized_text\"\n",
    "    eval_dataset = eval_dataset.remove_columns(\n",
    "        [col for col in eval_dataset.column_names if col not in [\"audio\", reference_col]]\n",
    "    )\n",
    "\n",
    "    # Audio is already 16kHz, casting is good practice but optional\n",
    "    eval_dataset = eval_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "    # Rename reference column\n",
    "    eval_dataset = eval_dataset.rename_column(reference_col, \"reference\")\n",
    "\n",
    "    print(\"Dataset subset preprocessed.\")\n",
    "    print(\"Example reference:\", eval_dataset[0]['reference'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or preprocessing dataset: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, processor, dataset, device, batch_size=8):\n",
    "    \"\"\"Evaluates a Whisper model on a dataset and returns WER and CER.\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # --- START: Corrected GenerationConfig Handling ---\n",
    "    # Use the generation_config attached to the model directly\n",
    "    generation_config = model.generation_config\n",
    "\n",
    "    # Override specific settings needed for evaluation\n",
    "    generation_config.language = TARGET_LANGUAGE\n",
    "    generation_config.task = TASK\n",
    "    generation_config.forced_decoder_ids = None # Ensure processor handles start tokens\n",
    "    generation_config.suppress_tokens = []    # Ensure no tokens are suppressed unless intended\n",
    "    # --- END: Corrected GenerationConfig Handling ---\n",
    "\n",
    "\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    stereo_count = 0\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    print(f\"Starting evaluation loop with batch size {batch_size}...\")\n",
    "    try:\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "            # --- NumPy Conversion & Mono Check ---\n",
    "            # (Keep the conversion logic from the previous step)\n",
    "            audio_arrays_raw = batch[\"audio\"][\"array\"]\n",
    "            processed_audio_np = []\n",
    "            for array_or_tensor in audio_arrays_raw:\n",
    "                if isinstance(array_or_tensor, torch.Tensor):\n",
    "                    array = array_or_tensor.numpy()\n",
    "                elif isinstance(array_or_tensor, np.ndarray):\n",
    "                    array = array_or_tensor\n",
    "                else: continue\n",
    "                if array.ndim > 1 and array.shape[0] > 1 :\n",
    "                    stereo_count += 1\n",
    "                    array = np.mean(array, axis=0)\n",
    "                processed_audio_np.append(array)\n",
    "            if not processed_audio_np: continue\n",
    "            # --- End Conversion ---\n",
    "\n",
    "            # --- START: Modify Processor Call & Mask Handling ---\n",
    "            # Prepare batch using the list of NumPy arrays\n",
    "            # The processor output is a BatchFeature object (dict-like)\n",
    "            processed_batch = processor(\n",
    "                processed_audio_np,\n",
    "                sampling_rate=batch[\"audio\"][\"sampling_rate\"][0].item(),\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Extract features and mask, move to device\n",
    "            input_features = processed_batch.input_features.to(device, dtype=MODEL_PRECISION)            \n",
    "            reference_texts = batch[\"reference\"]\n",
    "\n",
    "            # Generate predictions\n",
    "            with torch.inference_mode():\n",
    "                 with torch.autocast(device_type=device.type, dtype=MODEL_PRECISION):\n",
    "                    # Pass the modified generation_config explicitly\n",
    "                    predicted_ids = model.generate(input_features, generation_config=generation_config)\n",
    "\n",
    "            # Decode and normalize\n",
    "            predictions_decoded = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "            predictions_norm = [normalize_text(pred) for pred in predictions_decoded]\n",
    "            references_norm = [normalize_text(ref) for ref in reference_texts]\n",
    "\n",
    "            all_predictions.extend(predictions_norm)\n",
    "            all_references.extend(references_norm)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"\\n*** Error occurred during evaluation loop at batch {i}: {e} ***\")\n",
    "         print(\"--- Full Traceback ---\")\n",
    "         traceback.print_exc()\n",
    "         print(\"----------------------\")\n",
    "         print(\"Evaluation stopped prematurely due to error.\")\n",
    "         # Compute partial metrics only if lists are not empty\n",
    "         partial_wer = -1\n",
    "         partial_cer = -1\n",
    "         if all_references and all_predictions: # Check if lists have content\n",
    "             partial_wer = wer_metric.compute(predictions=all_predictions, references=all_references) * 100\n",
    "             partial_cer = cer_metric.compute(predictions=all_predictions, references=all_references) * 100\n",
    "             print(f\"Partial WER: {partial_wer:.2f}%, Partial CER: {partial_cer:.2f}% (on {len(all_predictions)} samples)\")\n",
    "         return {\"wer\": partial_wer, \"cer\": partial_cer, \"stereo_files_found\": stereo_count, \"error\": True}\n",
    "\n",
    "\n",
    "    # --- Report Stereo Count ---\n",
    "    print(f\"\\nProcessed {len(all_references)} samples. Found {stereo_count} potential stereo file(s) (converted to mono).\")\n",
    "\n",
    "    # Compute metrics if loop completes\n",
    "    print(\"Computing WER and CER...\")\n",
    "    wer = wer_metric.compute(predictions=all_predictions, references=all_references)\n",
    "    cer = cer_metric.compute(predictions=all_predictions, references=all_references)\n",
    "\n",
    "    results_dict = {\n",
    "        \"wer\": wer * 100,\n",
    "        \"cer\": cer * 100,\n",
    "        \"stereo_files_found\": stereo_count,\n",
    "        \"error\": False\n",
    "    }\n",
    "    print(\"Evaluation loop finished successfully.\")\n",
    "    return results_dict\n",
    "\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eefa52b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Base Model (language-and-voice-lab/whisper-large-icelandic-30k-steps-1000h) ---\n",
      "Loading base model...\n",
      "Base model loaded.\n",
      "Starting evaluation loop with batch size 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec8c785ebc141d1a3eeb8d5f521d7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 1000 samples. Found 0 potential stereo file(s) (converted to mono).\n",
      "Computing WER and CER...\n",
      "Evaluation loop finished successfully.\n",
      "Base Model Results: WER=7.73%, CER=3.62%\n",
      "  Stereo files found/converted: 0\n",
      "\n",
      "Unloading Base Model...\n",
      "GPU memory cleared.\n",
      "Model unloaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate Base Model ---\n",
    "print(f\"\\n--- Evaluating Base Model ({MODEL_ID}) ---\")\n",
    "model_key = \"Base Model\"\n",
    "# Default values in case of early failure (e.g., model loading)\n",
    "results[model_key] = {\n",
    "    \"wer\": float('inf'),\n",
    "    \"cer\": float('inf'),\n",
    "    \"stereo_files_found\": -1,\n",
    "    \"eval_error\": True # Assume error until proven otherwise\n",
    "}\n",
    "\n",
    "try:\n",
    "    # --- Model Loading ---\n",
    "    print(\"Loading base model...\")\n",
    "    base_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=MODEL_PRECISION,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=\"sdpa\" # Use consistent attention\n",
    "    )\n",
    "    base_model.to(DEVICE)\n",
    "    print(\"Base model loaded.\")\n",
    "\n",
    "    # --- Run Evaluation ---\n",
    "    # This call now returns the detailed dictionary\n",
    "    eval_metrics = evaluate_model(base_model, processor, eval_dataset, DEVICE, EVAL_BATCH_SIZE)\n",
    "\n",
    "    # --- Store Results ---\n",
    "    # Update the results dictionary with values from eval_metrics\n",
    "    # Use .get() with defaults in case eval_metrics is incomplete due to early error\n",
    "    results[model_key]['wer'] = eval_metrics.get(\"wer\", float('inf'))\n",
    "    results[model_key]['cer'] = eval_metrics.get(\"cer\", float('inf'))\n",
    "    results[model_key]['stereo_files_found'] = eval_metrics.get(\"stereo_files_found\", -1)\n",
    "    results[model_key]['eval_error'] = eval_metrics.get(\"error\", True) # Update error status\n",
    "\n",
    "    # Print key results\n",
    "    print(f\"{model_key} Results: WER={results[model_key]['wer']:.2f}%, CER={results[model_key]['cer']:.2f}%\")\n",
    "    if results[model_key]['eval_error']:\n",
    "        print(f\"  Warning: Evaluation for {model_key} may not have completed fully (error flag set).\")\n",
    "    print(f\"  Stereo files found/converted: {results[model_key]['stereo_files_found']}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch errors during loading or other setup BEFORE evaluate_model is called\n",
    "    print(f\"--- Error during {model_key} loading or setup: {e} ---\")\n",
    "    # Log traceback for unexpected errors\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    # results[model_key] is already set to error defaults\n",
    "    print(f\"{model_key} Results: Evaluation could not be run due to setup error.\")\n",
    "\n",
    "finally:\n",
    "    # --- Model Unloading ---\n",
    "    print(f\"\\nUnloading {model_key}...\")\n",
    "    if 'base_model' in locals(): del base_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared.\")\n",
    "    print(\"Model unloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f1441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Adapter Model (./whisper-lvl-base-raddromur-lora/checkpoint-216) ---\n",
      "Loading base model for adapter...\n",
      "Loading LoRA adapter...\n",
      "Adapter loaded.\n",
      "Starting evaluation loop with batch size 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79833a3e1b82458ba6ad1c93da740278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Evaluate Adapter Model ---\n",
    "print(f\"\\n--- Evaluating Adapter Model ({ADAPTER_CHECKPOINT_PATH}) ---\")\n",
    "model_key = \"Fine-tuned Adapter\"\n",
    "results[model_key] = { # Default error values\n",
    "    \"wer\": float('inf'), \"cer\": float('inf'), \"stereo_files_found\": -1, \"eval_error\": True\n",
    "}\n",
    "try:\n",
    "    print(\"Loading base model for adapter...\")\n",
    "    base_model_for_adapter = WhisperForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=MODEL_PRECISION,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=\"sdpa\"\n",
    "    )\n",
    "\n",
    "    print(\"Loading LoRA adapter...\")\n",
    "    peft_model = PeftModel.from_pretrained(base_model_for_adapter, ADAPTER_CHECKPOINT_PATH)\n",
    "    print(\"Adapter loaded.\")\n",
    "    peft_model.to(DEVICE)\n",
    "\n",
    "    results[\"Fine-tuned Adapter\"] = evaluate_model(peft_model, processor, eval_dataset, DEVICE, EVAL_BATCH_SIZE)\n",
    "    print(\"Adapter Model Results:\", results[\"Fine-tuned Adapter\"])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during adapter model evaluation: {e}\")\n",
    "    results[\"Fine-tuned Adapter\"] = {\"wer\": float('inf'), \"cer\": float('inf')}\n",
    "finally:\n",
    "    print(\"\\nUnloading adapter model...\")\n",
    "    if 'peft_model' in locals(): del peft_model\n",
    "    if 'base_model_for_adapter' in locals(): del base_model_for_adapter\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da984977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Present Results ---\n",
    "print(\"\\n--- Final Evaluation Results ---\")\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "# --- Plotting ---\n",
    "print(\"\\nGenerating comparison graph...\")\n",
    "try:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(len(results_df))\n",
    "\n",
    "    bar1 = ax.bar(index - bar_width/2, results_df['wer'], bar_width, label='WER (%)', color='skyblue')\n",
    "    bar2 = ax.bar(index + bar_width/2, results_df['cer'], bar_width, label='CER (%)', color='lightcoral')\n",
    "\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Error Rate (%)')\n",
    "    ax.set_title(f'Whisper Model Evaluation on Samrómur Milljón ({EVAL_SPLIT} subset)') # Updated title\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(results_df['Model'])\n",
    "    ax.legend()\n",
    "\n",
    "    ax.bar_label(bar1, padding=3, fmt='%.2f')\n",
    "    ax.bar_label(bar2, padding=3, fmt='%.2f')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"whisper_samromur_evaluation_comparison2.png\") # New filename\n",
    "    print(\"Graph saved as whisper_samromur_evaluation_comparison2.png\")\n",
    "    # plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating plot: {e}\")\n",
    "\n",
    "print(\"\\nEvaluation script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
